---
title: "LLM Inference on Edge: A Fun and Easy Guide to run LLMs via React Native on your Phone!"
date: "2026-02-14T22:29:41.820444"
source: "HuggingFace"
author: "Unknown"
link: "https://huggingface.co/blog/llm-inference-on-edge"
published: "Fri, 07 Mar 2025 00:00:00 GMT"
---

As LLMs continue to evolve, they are becoming smaller and smarter, enabling them to run directly on your phone. Take, for instance, the DeepSeek R1 Distil Qwen 2.5 with 1.5 billion parameters, this model really shows how advanced AI can now fit into the palm of your hand!
In this blog, we will guide you through creating a mobile app that allows you to chat with these powerful models locally. The complete code for this tutorial is available in our EdgeLLM repository.

[Read full article](https://huggingface.co/blog/llm-inference-on-edge)
