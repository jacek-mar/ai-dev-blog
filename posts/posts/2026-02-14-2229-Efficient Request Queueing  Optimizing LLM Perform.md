---
title: "Efficient Request Queueing â€“ Optimizing LLM Performance"
date: "2026-02-14T22:29:31.188818"
source: "HuggingFace"
author: "Unknown"
link: "https://huggingface.co/blog/tngtech/llm-performance-request-queueing"
published: "Wed, 02 Apr 2025 13:33:53 GMT"
---

Serving LLMs to many applications and users in parallel is challenging because they compete for limited GPU resources. This article is the first in a series on LLM performance, based on our experience with serving self-hosted LLMs at TNG Technology Consulting GmbH. In the first part, we focus on the impact of queuing and discuss different scheduling strategies.
An inference engine like vLLM or HuggingFace TGI consists of
Why do we need a queue here? Because calculations on the GPU are more perfo...

[Read full article](https://huggingface.co/blog/tngtech/llm-performance-request-queueing)
