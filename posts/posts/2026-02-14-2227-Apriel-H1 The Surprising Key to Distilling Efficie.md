---
title: "Apriel-H1: The Surprising Key to Distilling Efficient Reasoning Models"
date: "2026-02-14T22:27:58.587471"
source: "HuggingFace"
author: "Unknown"
link: "https://huggingface.co/blog/ServiceNow-AI/apriel-h1"
published: "Wed, 19 Nov 2025 05:19:07 GMT"
---

We converted our 15B reasoning model to a Mamba hybrid achieving 2.1x throughput with minimal quality loss. The key? A non-obvious insight about what data to distill on, and why intuition fails here.
When MiniMax published their M2 post-mortem in October explaining why they abandoned efficient attention at 230B scale, the narrative briefly became "efficient attention is dead." Within days, Kimi Linear proved otherwise.

[Read full article](https://huggingface.co/blog/ServiceNow-AI/apriel-h1)
