---
title: "BigCodeArena: Judging code generations end to end with code executions"
date: "2026-02-14T22:28:14.890306"
source: "HuggingFace"
author: "Unknown"
link: "https://huggingface.co/blog/bigcode/arena"
published: "Tue, 07 Oct 2025 09:37:25 GMT"
---

Evaluating the quality of AI-generated code is notoriously difficult. While humans can easily spot whether a piece of code "looks right," determining if it actually works correctly, handles edge cases properly, and produces the intended result requires running and testing it. This is why today, we're thrilled to announce BigCodeArena -- the first human-in-the-loop platform for evaluating code generation models through execution.
Inspired by LMArena for LLMs, we've built a platform that allows an...

[Read full article](https://huggingface.co/blog/bigcode/arena)
