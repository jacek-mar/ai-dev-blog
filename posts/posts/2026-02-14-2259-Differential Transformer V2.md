---
title: "Differential Transformer V2"
date: "2026-02-14T22:59:44.522612+00:00"
source: "HuggingFace"
author: "Unknown"
link: "https://huggingface.co/blog/microsoft/diff-attn-v2"
published: "Tue, 20 Jan 2026 03:20:57 GMT"
---

Tianzhu Ye, Li Dong, Yutao Sun, Furu Wei
Github Link
Notion Link (for better readability)
We introduce Differential Transformer V2 (DIFF V2), an improved version of Differential Transformer (DIFF V1). This revision focuses on inference efficiency, training stability for production-level LLMs, and architectural elegance.
Key improvements:
We conduct pretraining experiments on production-scale LLMs, including dense models and a 30A3 MoE on trillions of tokens using large learning rate of 6e-4 to 1...

[Read full article](https://huggingface.co/blog/microsoft/diff-attn-v2)
