---
title: "Mixture of Experts (MoEs) in Transformers"
date: "2026-02-26T12:54:18.187538+00:00"
source: "HuggingFace"
author: "Unknown"
link: "https://huggingface.co/blog/moe-transformers"
published: "Thu, 26 Feb 2026 00:00:00 GMT"
---

Over the past few years, scaling dense language models has driven most progress in LLMs. From early models like the original ULMFiT (~30M parameters) or GPT-2 (1.5B parameters, which at the time was considered "too dangerous to release" ðŸ§Œ), and eventually to todayâ€™s hundred-billionâ€“parameter systems, the recipe was simple: More data + more parameters gives better performance.

[Read full article](https://huggingface.co/blog/moe-transformers)
